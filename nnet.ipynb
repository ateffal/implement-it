{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement a neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "from scipy.special import expit, logit, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a simple neural network with one hidden layer with n neurones and an output layer\n",
    "class Simple_nnet:\n",
    "\n",
    "    def __init__(self, input_shape , hidden_layer_shape = 16, output_shape = 2) :\n",
    "        self.input_shape = input_shape\n",
    "        self.hidden_layer_shape = hidden_layer_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        # init wheights and bias for hidden layer with random numbers \n",
    "        self.h_weights = rd.rand(hidden_layer_shape, input_shape)\n",
    "        self.h_bias = rd.rand(hidden_layer_shape, 1)\n",
    "\n",
    "        # init wheights and bias for output layer with random numbers \n",
    "        self.o_weights = rd.rand(output_shape, hidden_layer_shape)\n",
    "        self.o_bias = rd.rand(output_shape, 1)\n",
    "\n",
    "    def process_input(self, input):\n",
    "        # reshape input\n",
    "        input = np.reshape(input, (input.shape[0], 1))\n",
    "        # calculate hidden layer neurones. @ operator do matrix multiplication\n",
    "        a1 = (self.h_weights @ input )+ self.h_bias\n",
    "        a1 = expit(a1) # apply sigmoid function\n",
    "\n",
    "        # calculate output layer. @ operator do matrix multiplication\n",
    "        a2 = (self.o_weights @ a1 )+ self.o_bias\n",
    "        a2 = softmax(a2) # apply softmax function\n",
    "        \n",
    "        return a2\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.input_shape) + \" --> \" + str(self.hidden_layer_shape) + \" --> \" + str(self.output_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet1 = Simple_nnet(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = np.array([1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56470204],\n",
       "       [0.43529796]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet1.process_input(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([10, 20])[0] + softmax([10, 20])[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function (the output layer has $ n_o $ neurones): \n",
    "\n",
    "$$\n",
    "  C =  \\frac{1}{2} \\sum_{i=1}^{n_o} (\\hat{y}_{i} - y_i)^2\n",
    "$$\n",
    "\n",
    "So : \n",
    "\n",
    "$$\n",
    "\\frac{\\partial{C}}{\\hat{y}_{i}} = \\hat{y}_{i} - y_i\n",
    "\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation for the last layer (output) : </br>\n",
    "$$ \n",
    "a_i^{o} = \\sigma (z_i^{o}) = \\hat{y}_{i} \n",
    "$$\n",
    "\n",
    "where :\n",
    "$$\n",
    "z_i^{o} = \\sum_{k=1}^{n_H} W_{i, k}^{o}.a_{k}^{H} + b_{i}^{o}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's calculate the gradients of cost with regards to bias $ b_i^{o} $ . <br>\n",
    "For this, we need to understand how each term depends on the other so that we can use the chain rule. <br>\n",
    "This notation : Y &rarr; X, mean that Y depends on X, and if it's the case, generally we calculate the derivative of Y with regards to X : $  \\frac{\\partial{Y}}{\\partial{X}} $. <br>\n",
    "In our case, we have these dependencies (see formulas above): C &rarr; $ a_{i}^{o} $ &rarr; $ z_{i}^{o} $ &rarr; $ b_{i}^{o} $. <br>\n",
    "So we can use the chain rule to calculate gradient of cost with regards to $ b_i $ : \n",
    "\n",
    "$$\n",
    "\n",
    "\\frac{\\partial{C}}{\\partial{b_{i}^{o}}} = \\frac{\\partial{C}}{\\partial{a_{i}^{o}}} \\times \\frac{\\partial{a_{i}^{o}}}{\\partial{z_{i}^{o}}} \\times \\frac{\\partial{z_{i}^{o}}}{\\partial{b_{i}^{o}}}\n",
    "\n",
    "$$\n",
    "\n",
    "Let's calculate each term in the right separately : \n",
    "\n",
    "$$\n",
    "\n",
    "\\frac{\\partial{C}}{\\partial{a_{i}^{o}}} = (a_{i}^{o} - y_{i})\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "\\frac{\\partial{a_{i}^{o}}}{\\partial{z_{i}^{o}}} = \\sigma^{'}(z_{i}^{o}) = \\sigma(z_{i}^{o}) \\times (1 - \\sigma(z_{i}^{o})) = a_{i}^{o} \\times (1 - a_{i}^{o})\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    " \\frac{\\partial{z_{i}^{o}}}{\\partial{b_{i}^{o}}} = 1\n",
    "$$\n",
    "So : \n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{C}}{\\partial{b_{i}^{o}}} = (a_{i}^{o} - y_{i}) \\times a_{i}^{o} \\times (1 - a_{i}^{o})\n",
    "$$\n",
    "NB : Remenber that : $ a_{i}^{o} = \\hat{y}_{i} $ \n",
    "The following function implements just this : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_output_bias(y_hat, y):\n",
    "\n",
    "    \"\"\"\n",
    "        Calculate the gradient of cost function with regards to the bias of output layer. \n",
    "        The cost function is : 1/2 * some(y_hat_i - y_i) , i = 1,...,n_o , n_o is the the length of output\n",
    "        y_hat : output vector\n",
    "        y : ground truth vector of the same lenth as y_hat\n",
    "    \"\"\"\n",
    "\n",
    "    return (y_hat-y)*y_hat*(1-y_hat)\n",
    "\n",
    "# test function\n",
    "y_hat = np.array([1, 2])\n",
    "y = np.array([0.5, 1.5])\n",
    "gradient_output_bias(y_hat, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to the gradients with regards to the weights of the output layer : \n",
    "$$\n",
    "\n",
    "\\frac{\\partial{C}}{\\partial{W_{i, k}^{o}}}\n",
    "\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we will again the rule chain : \n",
    "$$\n",
    "\n",
    "\\frac{\\partial{C}}{\\partial{W_{i, k}^{o}}} = \\frac{\\partial{C}}{\\partial{a_{i}^{o}}} \\times \\frac{\\partial{a_{i}^{o}}}{\\partial{z_{i}^{o}}} \\times \\frac{\\partial{z_{i}^{o}}}{\\partial{W_{i, k}^{o}}}\n",
    "\n",
    "$$\n",
    "\n",
    "The two first terms were already calculated (see above) : \n",
    "\n",
    "$$\n",
    "\n",
    "\\frac{\\partial{C}}{\\partial{a_{i}^{o}}} = (a_{i}^{o} - y_{i})\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial{a_{i}^{o}}}{\\partial{z_{i}^{o}}} =  a_{i}^{o} \\times (1 - a_{i}^{o})\n",
    "$$\n",
    "\n",
    "For the third term, we know that : $ z_i^{o} = \\sum_{k=1}^{n_H} W_{i, k}^{o}.a_{k}^{H} + b_{i}^{o} $, so : \n",
    "\n",
    "$$\n",
    "\n",
    "    \\frac{\\partial{z_{i}^{o}}}{\\partial{W_{i, k}^{o}}} = a_{k}^{H}\n",
    "$$\n",
    "\n",
    "Therfore : \n",
    "$$\n",
    "    \\frac{\\partial{C}}{\\partial{W_{i, k}^{o}}} = (a_{i}^{o} - y_{i}) \\times a_{i}^{o} \\times (1 - a_{i}^{o}) \\times a_{k}^{H}\n",
    "$$\n",
    "\n",
    "<b> NB : Remenber that : $ a_{i}^{o} = \\hat{y}_{i} $ </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement this calculations in a function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_output_weights(a_o, a_h, y):\n",
    "\n",
    "    \"\"\"\n",
    "        Calculate the gradient of cost function with regards to the weights of output layer. \n",
    "        a_o : vector of dimension no, activation of output ( = y_hat)\n",
    "        a_h : vector of dimension nh, activation of hidden layer\n",
    "        y : ground truth vector of the same lenth as y_hat\n",
    "    \"\"\"\n",
    "\n",
    "    # first we need to get dimensions of a_o and a_h\n",
    "    no = len(a_o) # equals dim of y \n",
    "    nh = len(a_h)\n",
    "\n",
    "    # intermidate calculs (multiplication of the 3 first terms in eq. above)\n",
    "    temp = (a_o - y) * a_o * (1 - a_o) # here we get a vector of length no\n",
    "\n",
    "    # reshape vectors to make matrix multiplication\n",
    "    temp = temp.reshape((no, 1))\n",
    "    a_h = a_h.reshape((1, nh))\n",
    "\n",
    "    return temp @ a_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  0. ,  0. ],\n",
       "       [-3. , -4. , -3.2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function\n",
    "a_o_1 = np.array([1, 2])\n",
    "y_1 = np.array([0.5, 1.9])\n",
    "a_h_1 = np.array([15, 20, 16])\n",
    "\n",
    "gradient_output_weights(a_o_1, a_h_1, y_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to the gradients with regards to the bias of hidden layer :\n",
    "$$\n",
    "\n",
    "\\frac{\\partial{C}}{\\partial{b_{i}^{H}}} , i = 1,... n_{o}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate this we need to how C depends on $ b_i^H $ $ i = 1,...n_o $ : <br>\n",
    "\n",
    "A change in  $ b_{i}^{H} $ changes $ z_{i}^{H} $ which changes  $ a_{i}^{H} $ which changes $ ( z_{1}^{o} $ to $ z_{n_o}^{o} )  $ which changes C  <br>\n",
    "\n",
    "\n",
    "This mean that a change in $ b_{i}^{H} $ will change all $  z_{j}^{o} $ from j = 1 to ${n_o}$  which result in change of C. <br>\n",
    "\n",
    "So we can write : \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\frac{\\partial{C}}{\\partial{b_{i}^{H}}} = \\frac{\\partial{C}}{\\partial{z_{1}^{o}}} \\times \\frac{\\partial{z_{1}^{o}}}{\\partial{b_{i}^{H}}} + \\frac{\\partial{C}}{\\partial{z_{2}^{o}}} \\times \\frac{\\partial{z_{2}^{o}}}{\\partial{b_{i}^{H}}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be written in a matricielle form : "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\frac{\\partial{C}}{\\partial{b^{H}}} = (\\frac{\\partial{C}}{\\partial{z^{o}}})^{trans} \\times \\frac{\\partial{z^{o}}}{\\partial{b^{H}}} \n",
    "$$\n",
    "which trans, stands for tranpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
